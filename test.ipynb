{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msa_toolbox.utils.image.load_data_and_models import load_dataset\n",
    "data = load_dataset(\"imagenet\", \"/mnt/disk1/sumdev/MSA_BTP/utils_train/data/Imagenet_full/Imagenet_2010_train/full_data\", transform=True)\n",
    "loader = DataLoader(data, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.Size([32]) torch.Size([32])\n",
      "torch.Size([3, 224, 224]) tensor(484) tensor(622595)\n",
      "/mnt/disk1/sumdev/MSA_BTP/utils_train/data/Imagenet_full/Imagenet_2010_train/full_data/n02981792.tar/n02981792_25938.JPEG 484\n"
     ]
    }
   ],
   "source": [
    "for image, label, index in loader:\n",
    "    print(image.shape, label.shape, index.shape)\n",
    "    img = image[0]\n",
    "    label = label[0]\n",
    "    idx = index[0]\n",
    "    print(img.shape, label, idx)\n",
    "    path, label = data.samples[idx]\n",
    "    print(path, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code: CONN_EXCEEDS_LIMITS\n",
      "description: \"Account limits exceeded\"\n",
      "details: \"Request exceeds \\\"Pre-built classification model predict ops\\\" limit. Please contact support@clarifai.com or upgrade your plan.\"\n",
      "req_id: \"26bf7998063a16d829229e4e996f3b00\"\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Post model outputs failed, status: Account limits exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_model_outputs_response\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m!=\u001b[39m status_code_pb2\u001b[38;5;241m.\u001b[39mSUCCESS:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(post_model_outputs_response\u001b[38;5;241m.\u001b[39mstatus)\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPost model outputs failed, status: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m post_model_outputs_response\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mdescription)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Since we have one input, one output will exist here\u001b[39;00m\n\u001b[1;32m     69\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mException\u001b[0m: Post model outputs failed, status: Account limits exceeded"
     ]
    }
   ],
   "source": [
    "##################################################################################################\n",
    "# In this section, we set the user authentication, user and app ID, model details, and the URL\n",
    "# of the image we want as an input. Change these strings to run your own example.\n",
    "#################################################################################################\n",
    "\n",
    "# Your PAT (Personal Access Token) can be found in the portal under Authentification\n",
    "PAT = 'd041eff2b344468586a5d6666beaa38c'\n",
    "# Specify the correct user_id/app_id     pairings\n",
    "# Since you're making inferences outside your app's scope\n",
    "USER_ID = 'clarifai'\n",
    "APP_ID = 'main'\n",
    "# Change these to whatever model and image URL you want to use\n",
    "MODEL_ID = 'moderation-recognition'\n",
    "MODEL_VERSION_ID = 'aa8be956dbaa4b7a858826a84253cab9'\n",
    "IMAGE_URL = 'https://samples.clarifai.com/metro-north.jpg'\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# YOU DO NOT NEED TO CHANGE ANYTHING BELOW THIS LINE TO RUN THIS EXAMPLE\n",
    "############################################################################\n",
    "\n",
    "from clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\n",
    "from clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\n",
    "from clarifai_grpc.grpc.api.status import status_code_pb2\n",
    "\n",
    "channel = ClarifaiChannel.get_grpc_channel()\n",
    "stub = service_pb2_grpc.V2Stub(channel)\n",
    "metadata = (('authorization', 'Key ' + PAT),)\n",
    "userDataObject = resources_pb2.UserAppIDSet(user_id=USER_ID, app_id=APP_ID)\n",
    "\n",
    "image_path = '/mnt/disk1/sumdev/MSA_BTP/utils_train/data/Imagenet_full/Imagenet_2010_train/full_data/n01667114.tar/n01667114_14780.JPEG'\n",
    "with open(image_path, \"rb\") as f:\n",
    "    file_bytes = f.read()\n",
    "\n",
    "post_model_outputs_response = stub.PostModelOutputs(\n",
    "    service_pb2.PostModelOutputsRequest(\n",
    "        user_app_id=userDataObject,  # The userDataObject is created in the overview and is required when using a PAT\n",
    "        model_id=MODEL_ID,\n",
    "        version_id=MODEL_VERSION_ID,  # This is optional. Defaults to the latest model version\n",
    "        inputs=[\n",
    "            resources_pb2.Input(\n",
    "                id = '1',\n",
    "                data=resources_pb2.Data(\n",
    "                    image=resources_pb2.Image(\n",
    "                        # url=IMAGE_URL\n",
    "                        base64=file_bytes,\n",
    "                        allow_duplicate_url=True\n",
    "                    )\n",
    "                )\n",
    "            ), \n",
    "            # resources_pb2.Input(\n",
    "            #     id = '2',\n",
    "            #     data=resources_pb2.Data(\n",
    "            #         image=resources_pb2.Image(\n",
    "            #             url=IMAGE_URL,\n",
    "            #             allow_duplicate_url=True\n",
    "            #         )\n",
    "            #     )\n",
    "            # )\n",
    "        ]\n",
    "    ),\n",
    "    metadata=metadata\n",
    ")\n",
    "if post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n",
    "    print(post_model_outputs_response.status)\n",
    "    raise Exception(\"Post model outputs failed, status: \" + post_model_outputs_response.status.description)\n",
    "\n",
    "# Since we have one input, one output will exist here\n",
    "class_to_idx = {}\n",
    "for i in range(len(post_model_outputs_response.outputs)):\n",
    "    output = post_model_outputs_response.outputs[i]\n",
    "    print(\"Predicted concepts for input %d\" % i)\n",
    "    print(output.data.concepts)\n",
    "    for concept in output.data.concepts:\n",
    "        class_to_idx[concept.name] = len(class_to_idx)\n",
    "        print(\"%s %.2f\" % (concept.name, concept.value))\n",
    "\n",
    "# Uncomment this line to print the full Response JSON\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'safe': 0, 'drug': 1, 'suggestive': 2, 'gore': 3, 'explicit': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.tensor([0.396, 0.604, 0.922])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'catamaran', 'score': 0.8138094544410706},\n",
       " {'label': 'trimaran', 'score': 0.10711844265460968},\n",
       " {'label': 'yawl', 'score': 0.0026023515965789557},\n",
       " {'label': 'schooner', 'score': 0.0007784378831274807},\n",
       " {'label': 'laptop, laptop computer', 'score': 0.0005865186103619635}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "\n",
    "img = Image.open(\"/mnt/disk1/sumdev/MSA_BTP/utils_train/data/Imagenet_full/Imagenet_2010_train/full_data/n02981792.tar/n02981792_25938.JPEG\")\n",
    "# classifier = pipeline(\"image-classification\", model=\"Falconsai/nsfw_image_detection\")\n",
    "classifier = pipeline(\"image-classification\", model=\"apple/mobilevit-small\")\n",
    "classifier(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "old_stdout = sys.stdout\n",
    "\n",
    "log_file = open(\"out/message.log\",\"w\")\n",
    "\n",
    "sys.stdout = log_file\n",
    "\n",
    "print(\"this will be written to message.log\")\n",
    "\n",
    "print(\"this will be written to message.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'endpoint_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/disk1/sumdev/MSA_BTP/utils_train/data/Imagenet_full/Imagenet_2010_train/full_data/n01667114.tar/n01667114_14780.JPEG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Call your model for predicting which object appears in this image.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m response \u001b[38;5;241m=\u001b[39m runtime\u001b[38;5;241m.\u001b[39minvoke_endpoint(\n\u001b[0;32m---> 14\u001b[0m     EndpointName\u001b[38;5;241m=\u001b[39m\u001b[43mendpoint_name\u001b[49m, \n\u001b[1;32m     15\u001b[0m     ContentType\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/x-image\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     16\u001b[0m     Body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbytearray\u001b[39m(img)\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# read the prediction result and parse the json\u001b[39;00m\n\u001b[1;32m     19\u001b[0m result \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'endpoint_name' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import boto3\n",
    "runtime = boto3.Session().client(service_name='sagemaker-runtime', region_name='us-east-1')\n",
    "\n",
    "# set the object categories array\n",
    "object_categories = ['AnkleBoot','Bag','Coat','Dress','Pullover','Sandal','Shirt','Sneaker','TShirtTop','Trouser']\n",
    "\n",
    "# Load the image bytes\n",
    "img = open('/mnt/disk1/sumdev/MSA_BTP/utils_train/data/Imagenet_full/Imagenet_2010_train/full_data/n01667114.tar/n01667114_14780.JPEG', 'rb').read()\n",
    "    \n",
    "# Call your model for predicting which object appears in this image.\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-image', \n",
    "    Body=bytearray(img)\n",
    ")\n",
    "# read the prediction result and parse the json\n",
    "result = response['Body'].read()\n",
    "result = json.loads(result)\n",
    "\n",
    "# which category has the highest confidence?\n",
    "pred_label_id = np.argmax(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
